6. 特征选择

我们在上一章中讲到了特征构建。通过特征构建可以从已知的数据样本中获得对问题进行建模的数学化的对象（即向量）。针对具体的业务问题，我们可以采用不同的模型来（近似）描述现实世界，不同的模型对数据的要求是不一样的，当尝试或者选定了合适的模型时，我们就需要从已有的特征中选择合适的特征来训练模型（有时特征不够，还要进行构建新特征或者特征处理、增强等操作），那么这些特征中哪些是适合用于构建模型的呢？这就是本章要讲解的特征选择问题。

特征选择是指从所有构建的特征中选择出一个特征子集，用于模型训练与模型学习的过程。特征选择不光要评估特征本身，更需要评估特征与模型的匹配度，评估特征对最终的预测目标的精准度的贡献。特征没有最好的，只有跟应用场景和模型匹配的，特征选择对于构建机器学习应用是非常重要的一环。特征选择主要有以下两个目的：

- 简化模型，节省存储和计算开销，让模型更易于理解和使用；
- 减少特征数量、降维，改善通用性、降低过拟合的风险。

知道了什么是特征选择以及特征选择的价值，下面我们提供进行特征选择的具体方法，主要有基于统计量的方法、基于模型的方法和基于业务的方法，下面我们分别介绍。

 **6.1 基于统计量的特征选择方法**

所谓基于统计量的选择方法，就是基于模型或者/和特征，计算一个/组统计量（数值、向量或者矩阵），基于统计量的具体情况来判断特征是否适合该模型。基于统计量的选择我们介绍如下6种方式，下面我们逐一展开说明。

**6.1.1 选择****方差大的特征**

方差反映了特征样本的分布情况，我们可以分析特征的数据分布。分布均匀的特征，样本之间差别不大，该特征不能很好区分不同样本，而分布不均匀的特征，样本之间有极大的区分度，因此通常可以选择方差较大的特征，剔除掉方差变化小的特征。具体方差多大算大，可以事先计算出所有特征的方差，选择一定比例(比如20%)的方差大的特征，或者可以设定一个阈值，选择方差大于阈值的特征。

如果上面的原则或者方法大家不好理解，我这里举个极端的例子说明，看完这个例子我相信大家都能明白。如果我们构建一个模型来预测硕士毕业生工作3年后的收入，假设性别是其中一个特征。如果训练样本中所有的人都是男性，那么男性这个特征就没有区分度（这个例子中性别特征的方差为0），因为所有人都是男性，那么这时性别特征对模型就没有任何价值。

 **6.1.2** **皮尔逊相关系数**

皮尔森相关系数是一种简单的，能帮助理解特征和目标变量之间关系的方法，用于衡量变量之间的线性相关性，取值区间为[-1，1]，-1 表示完全的负相关，+1 表示完全的正相关，0 表示没有线性关系（但是不代表就没关系，可能有非线性关系）。通过分析特征与目标之间的相关性，优先选择与目标相关性高的特征。如果两个特征之间线性相关度的绝对值大，说明这两个特征是有很强的相关关系的，我们没必要都选择，只需要选择其中一个即可。 

如果特征或者目标变量都是数值型特征的话，皮尔逊相关系数可以用于计算特征之间的相关性，也可以用于计算特征与目标变量的相关性。

 **6.1.3** **覆盖率**

特征的覆盖率是指训练样本中有多大比例的样本具备该特征（不具备的原因可能是收集数据时有无效值或者空值导致的）。我们首先计算每个特征的覆盖率，覆盖率很小的特征对模型的预测效果作用不大，可以剔除。

比如，如果年龄是一个特征的话，当大多数用户在注册时不填年龄，那么绝大多数样本的年龄都是空的，那么年龄这个特征的覆盖率就很低，就是一个无效特征。

 **6.1.4 假设检验**

假设特征变量和目标变量之间相互独立，选择适当检验方法计算统计量，然后根据统计量做出统计推断（推断变量之间是不是相关的）。例如对于特征变量为类别变量而目标变量为连续数值变量的情况，可以使用方差分析，对于特征变量和目标变量都为连续数值变量的情况，可以使用皮尔森卡方检验，卡方统计量取值越大，特征相关性越高。

 **6.1.5 互信息**

在概率论和信息论中，互信息用来度量两个变量之间的相关性。互信息越大则表明两个变量相关性越高，互信息为 0 时，两个变量相互独立。因此可以根据特征变量和目标变量之间的互信息来选择互信息大的特征。关于互信息的计算公式，读者可以参考相关文献资料，这里不细讲。

 **6.1.6 二分类模型的特征选择方法**

上面是几种基本的选择特征的方法，针对预测变量是离散的情况（即分类模型，比如预测用户是否点击，下面就以预测二分类模型为例来说明），这时可以先进行分桶再看特征与目标变量的相关性，通过这种方法进行筛选特征。这又分为如下两种情况：

 **6.1.6.1 特征是离散特征**

这时可以根据离散特征的不同取值将样本分为几组（比如性别特征可以分为两组：男、女），然后统计每组中目标变量为1的比例（二分类，用0、1表示），同时计算这组样本占总体样本的比例。最终获得如下表格：

| 特征类别 | 特征变量占总样本的比例 | 目标变量中1的比例 |
| -------- | ---------------------- | ----------------- |
| c1       | x1                     | y1                |
| ...      | ...                    | ...               |
| c_k      | x_k                    | y_k               |

那么我们就可以计算（x1，... ，x_k）与（y1，... ，y_k）的相关性。这个相关性就可以代表特征变量与目标变量之间的相关性。

其实上面的方法跟**信息增益**的方法本质上是类似的，只不过上面这个方法更直观更好理解。读者想相信了解信息增益方法的可以参考https://blog.csdn.net/It_BeeCoder/article/details/79554388这篇文章。

 上面这个方法也可以拓展到计算离散特征与其它连续特征之间的相关性。我们可以用其它特征分布在c1、c2、... 、c_k组中的数值的均值或者中位数组装成一个向量，那么就可以计算该向量与（x1，... ，x_k）的相关性。

 **6.1.6.2 特征是连续特征**

如果特征是连续变量，那么可以将特征列进行排序（注意：特征列要和预测列进行联动排序），然后可以等区间（每个组的区间范围差不多）或者等数量（每个组样本数量差不多）分组（不管用什么方法分组，一定要保证每组中样本数量足够多，这样才具备统计意义），用每组的均值（或者中位数）作为该组的值，该组对应的目标变量中1的比例作为目标变量的值，获得如下的表格：

| 特征的组别 | 该组中特征的均值或者中位数 | 该组中目标变量中1的比例 |
| ---------- | -------------------------- | ----------------------- |
| c1         | x1                         | y1                      |
| ...        | ...                        | ...                     |
| c_k        | x_k                        | y_k                     |

那么我们就可以计算（x1，... ，x_k）与（y1，... ，y_k）的相关性。这个相关性就可以代表特征变量与目标变量之间的相关性。

如果每个特征与目标变量的相关性计算出来了，那么我们可以利用相关性进行降序排列，确定一个阈值（比如相关性的绝对值>=0.6的特征）或者一个比例（比如用30%最相似的特征），选择相关性大的部分特征来构建模型。

上面讲完了基于统计量的选择方法，统计量的选择方法相对简单，计算过程不复杂，也比较容易理解，跟模型关系不大，所以精准度相对较低。下面来说说更复杂、更有针对性（与具体的模型相关）的特征选择方法。

**6.2 基于模型的特征选择方法**

基于模型的特征选择，就是特征选择的过程跟最终训练的模型是相关的，甚至是耦合在一起的，这种特征选择方法更有针对性，更精准，同时计算量也相对更高。基于模型的特征选择方法可以直接根据模型参数来选择，也可用子集选择的思路选出特征的最优组合。

**6.2.1 基于模型参数**

对于线性模型（如一般线性模型、logistics回归等），可以直接基于模型系数大小来决定特征的重要程度。一般系数绝对值越大（系数是正的代表与预测变量是正相关的，反之负相关），该特征对模型的重要性就越大，绝对值很小的特征就可以剔除掉。

 对于树模型，如决策树、梯度提升树、随机森林等，每一棵树的生成过程，都对应了一个特征选择的过程，在每次选择分类节点时，都会选择最佳分类特征来进行切分，重要的特征更有可能出现在树生成早期的节点，作为分裂节点的次数也越多。因此，可以基于树模型中特征出现次数等指标对特征重要性进行排序。scikit-learn中的树模型直接是可以输出特征重要性的，读者可以自行参考学习。

如果我们想要得到稀疏特征或者说是对特征进行降维，可以在模型上主动使用正则化技术。使用L1正则，调整正则项的权重，基本可以得到任意维度的稀疏特征。

**6.2.2 子集选择**

基于模型，我们也可以用子集选择的思路来选取特征。常见的有前向搜索和反向搜索两种思路。

如果我们先从N个特征中选出一个最好的特征，然后让其余的N-1个特征分别与第一次选出的特征进行组合，从N-1个二元特征组合中选出最优组合，然后在上次的基础上，添加另一个新的特征，考虑3个特征的组合，依次类推，这种方法叫做前向搜索。

反之，如果我们的目标是每次从已有特征中去掉一个特征，并从这些组合（即分别去掉一个不同的特征后的组合）中选出最优组合，这种方法就是反向搜索。如果特征数量较多、模型复杂，那么这种选择的过程是非常耗时间和资源的。

上面比较抽象地提到了最优组合，那么什么是最优组合呢？一般机器学习模型就是一个最优化模型，都是有目标函数的，如果我们的目标函数是损失函数，那么最优的模型就是让损失函数最小的模型。所以针对上面的两个方法（即前向搜索和反向搜索），加上一个特征或者剔除一个特征后，能让损失函数最小的特征就是我们需要增加或者剔除的特征。 

上面是两种最常见的基于模型的特征选择方法。随着AutoML（自动化机器学习）技术的进展，目前也有非常多的自动化特征选择技术。AutoML试图将这些与特征、模型、优化、评价有关的重要步骤自动化，使得机器学习模型无需人工干预即可自动化地学习与训练。针对自动化特征工程相关的技术，有兴趣的读者可以自行查找相关文章、书籍学习，这里不赘述。（这里提供一本相关专著供大家参考：机械工业出版社的《深入理解AutoML和AutoDL》）

**6.3 基于业务的特征选择方法**

这个方法可能不需要太多的技术，如果你对建模的问题的本质有比较好的理解，那么你是知道什么变量对最终的目标函数是有价值的，那么这个特征一定是重要的。

一般来说，如果你对业务背景比较熟悉，根据专业知识和经验，你是可以非常容易选择一些非常有价值的特征的。比如如果预测大学毕业生毕业3年之后的年收入，那么大家肯定知道学校和学历一定是两个重要的特征。

所以，算法工程师还是需要非常懂业务的，这样可以方便地帮助你在特征的海洋（很多公司的特征仓库非常大，所以才这么说）中最准确、最快速地选择最优质的的特征。由于这个方法跟问题的领域及场景相关，这里无法很详细的说明，但是这一定是最重要的一种特征选择方法，大家一定要重视。

**总结**

本章我们简单介绍了3类进行特征选择的方法：基于统计量的方法、基于模型的方法和基于业务的方法。大家对这些方法的基本原理需要了解和掌握。关键还是要在实际项目中多尝试、多使用，这样就可以更好地理解特征选择的原理、步骤和价值。只有不断地实践，才能真正掌握各种方法的精髓。

特征选择是构建机器学习模型中非常重要、必不可少的一环，只有好的特征才能构建效果达标的模型。每个从事机器学习工作的人都应该掌握特征选择的基础方法。下一章我们会讲解特征评估。