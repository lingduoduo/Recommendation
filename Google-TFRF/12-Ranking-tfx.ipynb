{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d3e365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.5.2\n",
      "TFX version: 1.0.0\n",
      "Tensorflow Recommenders version: v0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "import tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import tensorflow_recommenders as tfrs\n",
    "print('Tensorflow Recommenders version: {}'.format(tfrs.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb3ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b70be3",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ce9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = 'TFRS-ranking'\n",
    "\n",
    "# Directory where MovieLens 100K rating data lives\n",
    "DATA_ROOT = os.path.join('data', PIPELINE_NAME)\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b4876",
   "metadata": {},
   "source": [
    "### Prepare example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ececa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-05 14:34:34--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "ml-100k.zip         100%[===================>]   4.70M  7.28MB/s    in 0.6s    \n",
      "\n",
      "2022-04-05 14:34:35 (7.28 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!mkdir -p {DATA_ROOT}\n",
    "!unzip ml-100k.zip\n",
    "!echo 'userId,movieId,rating,timestamp' > {DATA_ROOT}/ratings.csv\n",
    "!sed 's/\\t/,/g' ml-100k/u.data >> {DATA_ROOT}/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4b5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r\n",
      "196,242,3,881250949\r\n",
      "186,302,3,891717742\r\n",
      "22,377,1,878887116\r\n",
      "244,51,2,880606923\r\n",
      "166,346,1,886397596\r\n",
      "298,474,4,884182806\r\n",
      "115,265,2,881171488\r\n",
      "253,465,5,891628467\r\n",
      "305,451,3,886324817\r\n"
     ]
    }
   ],
   "source": [
    "!head {DATA_ROOT}/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce8bba",
   "metadata": {},
   "source": [
    "TFX pipelines are defined using Python APIs. We will define a pipeline which consists of following three components.\n",
    "\n",
    "- CsvExampleGen: Reads in data files and convert them to TFX internal format for further processing. There are multiple ExampleGens for various formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
    "\n",
    "- Trainer: Trains an ML model. Trainer component requires a model definition code from users. You can use TensorFlow APIs to specify how to train a model and save it in a _savedmodel format.\n",
    "\n",
    "- Pusher: Copies the trained model outside of the TFX pipeline. Pusher component can be thought of an deployment process of the trained ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69737b05",
   "metadata": {},
   "source": [
    "### Write model training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fe8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'tfrs_ranking_trainer.py'\n",
    "%%writefile {_trainer_module_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48000778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "_FEATURE_KEYS = ['userId', 'movieId']\n",
    "_LABEL_KEY = 'rating'\n",
    "\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e95a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        unique_user_ids = np.array(range(943)).astype(str)\n",
    "        unique_movie_ids = np.array(range(1682)).astype(str)\n",
    "\n",
    "        # Compute embeddings for users.\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(1,), name='userId', dtype=tf.int64),\n",
    "            tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        \n",
    "        # Compute embeddings for movies.\n",
    "        self.movie_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(1,), name='movieId', dtype=tf.int64),\n",
    "            tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_movie_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(\n",
    "                len(unique_movie_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute predictions.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        user_id, movie_id = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        movie_embedding = self.movie_embeddings(movie_id)\n",
    "\n",
    "        return self.ratings(tf.concat([user_embedding, movie_embedding], axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a04b9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ranking_model: tf.keras.Model = RankingModel()\n",
    "\t\tself.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "\t\t\tloss = tf.keras.losses.MeanSquaredError(),\n",
    "\t\t\tmetrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "\t\treturn self.ranking_model((features['userId'], features['movieId']))\n",
    "\n",
    "\tdef compute_loss(self,\n",
    "\t                 features: Dict[Text, tf.Tensor],\n",
    "\t                 training = False) -> tf.Tensor:\n",
    "\t\tlabels = features[1]\n",
    "\t\trating_predictions = self(features[0])\n",
    "\n",
    "\t\t# The task computes the loss and the metrics.\n",
    "\t\treturn self.task(labels = labels, predictions = rating_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d510a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _input_fn(file_pattern,\n",
    "              data_accessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 256) -> tf.data.Dataset:\n",
    "    \n",
    "\treturn data_accessor.tf_dataset_factory(\n",
    "\t\tfile_pattern,\n",
    "\t\ttfxio.TensorFlowDatasetOptions(\n",
    "\t\t\tbatch_size = batch_size, label_key = _LABEL_KEY\n",
    "\t\t),\n",
    "\t\tschema = schema\n",
    "\t).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "\treturn MovielensModel()\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args):\n",
    "\t\"\"\"Train the model based on given args.\n",
    "  \n",
    "\tArgs:\n",
    "\t  fn_args: Holds args used to train the model as name/value pairs.\n",
    "\t\"\"\"\n",
    "\tschema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "\ttrain_dataset = _input_fn(\n",
    "\t\tfn_args.train_files, fn_args.data_accessor, schema, batch_size = 8192\n",
    "\t)\n",
    "\teval_dataset = _input_fn(\n",
    "\t\tfn_args.eval_files, fn_args.data_accessor, schema, batch_size = 4096\n",
    "\t)\n",
    "\n",
    "\tmodel = _build_keras_model()\n",
    "\n",
    "\tmodel.compile(optimizer = tf.keras.optimizers.Adagrad(learning_rate = 0.1))\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\ttrain_dataset,\n",
    "\t\tsteps_per_epoch = fn_args.train_steps,\n",
    "\t\tepochs = 3,\n",
    "\t\tvalidation_data = eval_dataset,\n",
    "\t\tvalidation_steps = fn_args.eval_steps\n",
    "\t)\n",
    "\n",
    "\tmodel.save(fn_args.serving_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eca3f2",
   "metadata": {},
   "source": [
    "### Write a pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f6e4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str):\n",
    "\t\"\"\"Creates a three component pipeline with TFX.\"\"\"\n",
    "\t# Brings data into the pipeline.\n",
    "\texample_gen = tfx.components.CsvExampleGen(input_base = data_root)\n",
    "\n",
    "\t# Uses user-provided Python function that trains a model.\n",
    "\ttrainer = tfx.components.Trainer(\n",
    "\t\tmodule_file = module_file,\n",
    "\t\texamples = example_gen.outputs['examples'],\n",
    "\t\ttrain_args = tfx.proto.TrainArgs(num_steps = 12),\n",
    "\t\teval_args = tfx.proto.EvalArgs(num_steps = 24)\n",
    "\t)\n",
    "\n",
    "\t# Pushes the model to a filesystem destination.\n",
    "\tpusher = tfx.components.Pusher(\n",
    "\t\tmodel = trainer.outputs['model'],\n",
    "\t\tpush_destination = tfx.proto.PushDestination(\n",
    "\t\t\tfilesystem = tfx.proto.PushDestination.Filesystem(\n",
    "\t\t\t\tbase_directory = serving_model_dir\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "\t# Following three components will be included in the pipeline.\n",
    "\tcomponents = [\n",
    "\t\texample_gen,\n",
    "\t\ttrainer,\n",
    "\t\tpusher,\n",
    "\t]\n",
    "\n",
    "\treturn tfx.dsl.Pipeline(\n",
    "\t\tpipeline_name = pipeline_name,\n",
    "\t\tpipeline_root = pipeline_root,\n",
    "\t\tmetadata_connection_config = tfx.orchestration.metadata\n",
    "\t\t\t.sqlite_metadata_connection_config(metadata_path),\n",
    "\t\tcomponents = components\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a9877",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab4e5f",
   "metadata": {},
   "source": [
    "TFX supports multiple orchestrators to run pipelines. In this tutorial we will use LocalDagRunner which is included in the TFX Python package and runs pipelines on local environment.\n",
    "\n",
    "Now we create a LocalDagRunner and pass a Pipeline object created from the function we already defined.\n",
    "\n",
    "The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d99262",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dd7bc",
   "metadata": {},
   "source": [
    "You should see \"INFO:absl:Component Pusher is finished.\" at the end of the logs if the pipeline finished successfully. Because Pusher component is the last component of the pipeline.\n",
    "\n",
    "The pusher component pushes the trained model to the SERVING_MODEL_DIR which is the serving_model/TFRS-ranking directory if you did not change the variables in the previous steps. You can see the result from the file browser in the left-side panel in Colab, or using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in created model directory.\n",
    "!ls -R {SERVING_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Load the latest model for testing\n",
    "loaded = tf.saved_model.load(max(glob.glob(os.path.join(SERVING_MODEL_DIR, '*/')), key=os.path.getmtime))\n",
    "print(loaded({'userId': [[42]], 'movieId': [[15]]}).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43cb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2.5] *",
   "language": "python",
   "name": "conda-env-tensorflow2.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
