{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49740df9",
   "metadata": {},
   "source": [
    "### Retrieval by FM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e381b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}(\\mathbf{x}):=w_0+\\sum_{i=1}^n w_i x_i+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathbf{v}_i, \\mathbf{v}_j\\right\\rangle x_i x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136ea40",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^n v_{i, f} x_i\\right)^2-\\sum_{i=1}^n v_{i, f}^2 x_i^2\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{u \\in U} v_{u, f} x_u+\\sum_{t \\in I} v_{t, f} x_t\\right)^2-\\sum_{u \\in U} v_{u, f}^2 x_u^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{u \\in U} v_{u, f} x_u\\right)^2+\\left(\\sum_{t \\in I} v_{t, f} x_t\\right)^2+2 \\sum_{u \\in U} v_{u, f} x_u \\sum_{t \\in I} v_{t, f} x_t-\\sum_{u \\in U} v_{u, f}^2 x_u^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0b8ee",
   "metadata": {},
   "source": [
    "用户向量：\n",
    "\n",
    "用户向量由两项表达式拼接得到。\n",
    "* 第一项为常数 1\n",
    "* 第二项是将用户相关的特征向量进行 sum pooling\n",
    "\n",
    "$$\n",
    "V_{\\text {user }}=\\left[1 ; \\quad \\sum_{u \\in U} v_u x_u\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dac87c",
   "metadata": {},
   "source": [
    "物品向量：\n",
    "\n",
    "用户向量由两项表达式拼接得到。\n",
    "* 第一项表示物品相关特征向量的一阶、二阶特征交互。\n",
    "* 第二项是将物品相关的特征向量进行 sum pooling 。\n",
    "\n",
    "$$\n",
    "V_{item}=\\left[\\sum_{t \\in I} w_t x_t+\\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{t \\in I} v_{t, f} x_t\\right)^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right) ; \\quad \\sum_{t \\in I} v_t x_t\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c6ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "  print(\"{}: {}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5014daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22c7d0",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16780a",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{1}{T} \\sum_{t=1}^T \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_t ; \\theta\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e72195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (word vector length, )\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    for all words in vocab (tranpose of U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    # centerWordVec:  (embedding_dim,1)\n",
    "    # outsideVectors: (vocab_size,embedding_dim)\n",
    "\n",
    "    scores = np.matmul(outsideVectors, centerWordVec)  # size=(vocab_size, 1)\n",
    "    probs = softmax(scores)  # size=(vocab, 1)\n",
    "\n",
    "    loss = -np.log(probs[outsideWordIdx])  # scalar\n",
    "\n",
    "    dscores = probs.copy()  # size=(vocab, 1)\n",
    "    dscores[outsideWordIdx] = dscores[outsideWordIdx] - 1  # dscores=y_hat - y\n",
    "    gradCenterVec = np.matmul(outsideVectors, dscores)  # J关于vc的偏导数公式  size=(vocab_size, 1)\n",
    "    gradOutsideVecs = np.outer(dscores, centerWordVec)  # J关于u的偏导数公式  size=(vocab_size, embedding_dim)\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dcefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "  \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    gradCenterVec =np.zeros(centerWordVec.shape)  # (embedding_size,1)\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)  # (vocab_size, embedding_size)\n",
    "    loss = 0.0\n",
    "\n",
    "    u_o = outsideVectors[outsideWordIdx]  # size=(embedding_size,1)\n",
    "    z = sigmoid(np.dot(u_o, centerWordVec))  # size=(1, )\n",
    "    loss -= np.log(z) # 损失函数的第一部分\n",
    "    gradCenterVec += u_o * (z - 1)   # J关于vc的偏导数的第一部分\n",
    "    gradOutsideVecs[outsideWordIdx] = centerWordVec * (z - 1)  # J关于u_o的偏导数计算\n",
    "\n",
    "    for i in range(K):\n",
    "        neg_id = indices[1 + i]\n",
    "        u_k = outsideVectors[neg_id]\n",
    "        z = sigmoid(-np.dot(u_k, centerWordVec))\n",
    "        loss -= np.log(z)\n",
    "        gradCenterVec += u_k * (1-z)\n",
    "        gradOutsideVecs[neg_id] += centerWordVec * (1 - z)\n",
    "\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cedd6d",
   "metadata": {},
   "source": [
    "### AirBnB Listing Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80d430",
   "metadata": {},
   "source": [
    "Real-time Personalization using Embeddings for Search Ranking at Airbnb\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/133566801\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/43295545"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462340c1",
   "metadata": {},
   "source": [
    "### YouTubeDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812acaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
