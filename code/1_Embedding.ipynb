{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49740df9",
   "metadata": {},
   "source": [
    "### Retrieval by FM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e381b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}(\\mathbf{x}):=w_0+\\sum_{i=1}^n w_i x_i+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathbf{v}_i, \\mathbf{v}_j\\right\\rangle x_i x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136ea40",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^n v_{i, f} x_i\\right)^2-\\sum_{i=1}^n v_{i, f}^2 x_i^2\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{u \\in U} v_{u, f} x_u+\\sum_{t \\in I} v_{t, f} x_t\\right)^2-\\sum_{u \\in U} v_{u, f}^2 x_u^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right) \\\\\n",
    "= & \\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{u \\in U} v_{u, f} x_u\\right)^2+\\left(\\sum_{t \\in I} v_{t, f} x_t\\right)^2+2 \\sum_{u \\in U} v_{u, f} x_u \\sum_{t \\in I} v_{t, f} x_t-\\sum_{u \\in U} v_{u, f}^2 x_u^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0b8ee",
   "metadata": {},
   "source": [
    "用户向量：\n",
    "\n",
    "用户向量由两项表达式拼接得到。\n",
    "* 第一项为常数 1\n",
    "* 第二项是将用户相关的特征向量进行 sum pooling\n",
    "\n",
    "$$\n",
    "V_{\\text {user }}=\\left[1 ; \\quad \\sum_{u \\in U} v_u x_u\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dac87c",
   "metadata": {},
   "source": [
    "物品向量：\n",
    "\n",
    "用户向量由两项表达式拼接得到。\n",
    "* 第一项表示物品相关特征向量的一阶、二阶特征交互。\n",
    "* 第二项是将物品相关的特征向量进行 sum pooling 。\n",
    "\n",
    "$$\n",
    "V_{item}=\\left[\\sum_{t \\in I} w_t x_t+\\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{t \\in I} v_{t, f} x_t\\right)^2-\\sum_{t \\in I} v_{t, f}^2 x_t^2\\right) ; \\quad \\sum_{t \\in I} v_t x_t\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ec3fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:54:30.806691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random, math, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d81f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评价指标\n",
    "# 推荐系统推荐正确的商品数量占用户实际点击的商品数量\n",
    "def Recall(Rec_dict, Val_dict):\n",
    "    '''\n",
    "    Rec_dict: 推荐算法返回的推荐列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...} \n",
    "    Val_dict: 用户实际点击的商品列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...}\n",
    "    '''\n",
    "    hit_items = 0\n",
    "    all_items = 0\n",
    "    for uid, items in Val_dict.items():\n",
    "        rel_set = items\n",
    "        rec_set = Rec_dict[uid]\n",
    "        for item in rec_set:\n",
    "            if item in rel_set:\n",
    "                hit_items += 1\n",
    "        all_items += len(rel_set)\n",
    "\n",
    "    return round(hit_items / all_items * 100, 2)\n",
    "\n",
    "# 推荐系统推荐正确的商品数量占给用户实际推荐的商品数\n",
    "def Precision(Rec_dict, Val_dict):\n",
    "    '''\n",
    "    Rec_dict: 推荐算法返回的推荐列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...} \n",
    "    Val_dict: 用户实际点击的商品列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...}\n",
    "    '''\n",
    "    hit_items = 0\n",
    "    all_items = 0\n",
    "    for uid, items in Val_dict.items():\n",
    "        rel_set = items\n",
    "        rec_set = Rec_dict[uid]\n",
    "        for item in rec_set:\n",
    "            if item in rel_set:\n",
    "                hit_items += 1\n",
    "        all_items += len(rec_set)\n",
    "\n",
    "    return round(hit_items / all_items * 100, 2)\n",
    "\n",
    "# 所有被推荐的用户中,推荐的商品数量占这些用户实际被点击的商品数量\n",
    "def Coverage(Rec_dict, Trn_dict):\n",
    "    '''\n",
    "    Rec_dict: 推荐算法返回的推荐列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...} \n",
    "    Trn_dict: 训练集用户实际点击的商品列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...}\n",
    "    '''\n",
    "    rec_items = set()\n",
    "    all_items = set()\n",
    "    for uid in Rec_dict:\n",
    "        for item in Trn_dict[uid]:\n",
    "            all_items.add(item)\n",
    "        for item in Rec_dict[uid]:\n",
    "            rec_items.add(item)\n",
    "    return round(len(rec_items) / len(all_items) * 100, 2)\n",
    "\n",
    "# 使用平均流行度度量新颖度,如果平均流行度很高(即推荐的商品比较热门),说明推荐的新颖度比较低\n",
    "def Popularity(Rec_dict, Trn_dict):\n",
    "    '''\n",
    "    Rec_dict: 推荐算法返回的推荐列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...} \n",
    "    Trn_dict: 训练集用户实际点击的商品列表, 形式:{uid: {item1, item2,...}, uid: {item1, item2,...}, ...}\n",
    "    '''\n",
    "    pop_items = {}\n",
    "    for uid in Trn_dict:\n",
    "        for item in Trn_dict[uid]:\n",
    "            if item not in pop_items:\n",
    "                pop_items[item] = 0\n",
    "            pop_items[item] += 1\n",
    "    \n",
    "    pop, num = 0, 0\n",
    "    for uid in Rec_dict:\n",
    "        for item in Rec_dict[uid]:\n",
    "            pop += math.log(pop_items[item] + 1) # 物品流行度分布满足长尾分布,取对数可以使得平均值更稳定\n",
    "            num += 1  \n",
    "    return round(pop / num, 3)\n",
    "\n",
    "# 将几个评价指标指标函数一起调用\n",
    "def rec_eval(val_rec_items, val_user_items, trn_user_items):\n",
    "    print('recall:',Recall(val_rec_items, val_user_items))\n",
    "    print('precision',Precision(val_rec_items, val_user_items))\n",
    "    print('coverage',Coverage(val_rec_items, trn_user_items))\n",
    "    print('Popularity',Popularity(val_rec_items, trn_user_items)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5b5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_path):\n",
    "    # 读取数据时，定义的列名\n",
    "    rnames = ['user_id','movie_id','rating','timestamp']\n",
    "    data = pd.read_csv(os.path.join(root_path, 'ratings.dat'), sep='::', engine='python', names=rnames)\n",
    "\n",
    "    lbe = LabelEncoder()\n",
    "    data['user_id'] = lbe.fit_transform(data['user_id'])\n",
    "    data['movie_id'] = lbe.fit_transform(data['movie_id']) \n",
    "\n",
    "    # 直接这么分是不是可能会存在验证集中的用户或者商品不在训练集合中呢？那这种的操作一半是怎么进行划分\n",
    "    trn_data_, val_data_, _, _ = train_test_split(data, data, test_size=0.2)\n",
    "\n",
    "    trn_data = trn_data_.groupby('user_id')['movie_id'].apply(list).reset_index()\n",
    "    val_data = val_data_.groupby('user_id')['movie_id'].apply(list).reset_index()\n",
    "\n",
    "    trn_user_items = {}\n",
    "    val_user_items = {}\n",
    "    \n",
    "    # 将数组构造成字典的形式{user_id: [item_id1, item_id2,...,item_idn]}\n",
    "    for user, movies in zip(*(list(trn_data['user_id']), list(trn_data['movie_id']))):\n",
    "        trn_user_items[user] = set(movies)\n",
    "\n",
    "    for user, movies in zip(*(list(val_data['user_id']), list(val_data['movie_id']))):\n",
    "        val_user_items[user] = set(movies)\n",
    "\n",
    "    return trn_user_items, val_user_items, trn_data_, val_data_, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfa8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩阵分解模型\n",
    "def MF(n_users, n_items, embedding_dim=8):\n",
    "    K.clear_session()\n",
    "    input_users = Input(shape=[1, ])\n",
    "    users_emb = Embedding(n_users, embedding_dim)(input_users)\n",
    "    \n",
    "    input_movies = Input(shape=[1, ])\n",
    "    movies_emb = Embedding(n_items, embedding_dim)(input_movies)\n",
    "    \n",
    "    users = BatchNormalization()(users_emb)\n",
    "    users = Reshape((embedding_dim, ))(users)\n",
    "    \n",
    "    movies = BatchNormalization()(movies_emb)\n",
    "    movies = Reshape((embedding_dim, ))(movies)\n",
    "    \n",
    "    output = Dot(1)([users, movies])\n",
    "    model = Model(inputs=[input_users, input_movies], outputs=output)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    \n",
    "    # 为了方便获取模型中的某些层，进行如下属性设置\n",
    "    model.__setattr__('user_input', input_users)\n",
    "    model.__setattr__('user_embedding', users_emb)\n",
    "    model.__setattr__('movie_input', input_movies)\n",
    "    model.__setattr__('movie_embedding', movies_emb)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6834589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K表示最终给用户推荐的商品数量，N表示候选推荐商品为用户交互过的商品相似商品的数量\n",
    "k = 80\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e836820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "root_path = '../data/ml-1m/'\n",
    "trn_user_items, val_user_items, trn_data, val_data, data = get_data(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存的名称\n",
    "# 定义模型训练时监控的相关参数\n",
    "model_path = 'mf.h5'\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                             mode='min', save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='min')\n",
    "callbacks = [checkpoint, reduce_lr, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43ca2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 64)                386560    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 1, 64)                237184    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1, 64)                256       ['embedding[0][0]']           \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1, 64)                256       ['embedding_1[0][0]']         \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 64)                   0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 64)                   0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1)                    0         ['reshape[0][0]',             \n",
      "                                                                     'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624256 (2.38 MB)\n",
      "Trainable params: 624000 (2.38 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 计算user和item的数量\n",
    "n_users = trn_data['user_id'].max() + 1\n",
    "n_items = trn_data['movie_id'].max() + 1\n",
    "embedding_dim = 64 # 用户及商品的向量维度\n",
    "model = MF(n_users, n_items, embedding_dim) # 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3acaa299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2809/2814 [============================>.] - ETA: 0s - loss: 3.6828\n",
      "Epoch 1: val_loss improved from inf to 0.91740, saving model to mf.h5\n",
      "2814/2814 [==============================] - 13s 4ms/step - loss: 3.6787 - val_loss: 0.9174 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# 模型的输入是user_id和movie_id\n",
    "hist = model.fit([trn_data['user_id'].values, trn_data['movie_id'].values], \n",
    "                trn_data['rating'].values, batch_size=256, epochs=1, validation_split=0.1,\n",
    "                callbacks=callbacks, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113580a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型的Embedding层\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.movie_input, outputs=model.movie_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec0befb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将验证集中的user_id进行排序,方便与faiss搜索的结果进行对应\n",
    "val_uids = np.array(sorted(val_data['user_id'].unique()))\n",
    "trn_items = np.array(sorted(trn_data['movie_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79e39a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练数据的实际索引与相对索引，\n",
    "# 实际索引指的是数据中原始的user_id\n",
    "# 相对索引指的是，排序后的位置索引，这个对应的是faiss库搜索得到的结果索引\n",
    "trn_items_dict = {}\n",
    "for i, item in enumerate(trn_items):\n",
    "    trn_items_dict[i] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c88d57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items_dict = set(trn_data['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68665452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step\n",
      "15/15 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "user_embs = user_embedding_model.predict([val_uids], batch_size=256).squeeze(axis=1)\n",
    "item_embs = item_embedding_model.predict([trn_items], batch_size=256).squeeze(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b51f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用向量搜索库进行最近邻搜索\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(item_embs)\n",
    "# ascontiguousarray函数将一个内存不连续存储的数组转换为内存连续存储的数组，使得运行速度更快。\n",
    "D, I = index.search(np.ascontiguousarray(user_embs), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4792343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将推荐结果转换成可以计算评价指标的格式\n",
    "# 选择最相似的TopN个item\n",
    "val_rec = {}\n",
    "for i, u in enumerate(val_uids):\n",
    "    items = list(map(lambda x: trn_items_dict[x], list(I[i]))) # 先将相对索引转换成原数据中的user_id\n",
    "    items = list(filter(lambda x: x not in trn_user_items[u], items))[:N] # 过滤掉用户在训练集中交互过的商品id，并选择相似度最高的前N个\n",
    "    val_rec[u] = set(items) # 将结果转换成统一的形式，便于计算模型的性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "540e3eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.04\n",
      "precision 0.13\n",
      "coverage 70.65\n",
      "Popularity 2.639\n"
     ]
    }
   ],
   "source": [
    "# 计算评价指标\n",
    "rec_eval(val_rec, val_user_items, trn_user_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cbd6",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c6ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "  print(\"{}: {}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5014daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16780a",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{1}{T} \\sum_{t=1}^T \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_t ; \\theta\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e72195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (word vector length, )\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    for all words in vocab (tranpose of U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    # centerWordVec:  (embedding_dim,1)\n",
    "    # outsideVectors: (vocab_size,embedding_dim)\n",
    "\n",
    "    scores = np.matmul(outsideVectors, centerWordVec)  # size=(vocab_size, 1)\n",
    "    probs = softmax(scores)  # size=(vocab, 1)\n",
    "\n",
    "    loss = -np.log(probs[outsideWordIdx])  # scalar\n",
    "\n",
    "    dscores = probs.copy()  # size=(vocab, 1)\n",
    "    dscores[outsideWordIdx] = dscores[outsideWordIdx] - 1  # dscores=y_hat - y\n",
    "    gradCenterVec = np.matmul(outsideVectors, dscores)  # J关于vc的偏导数公式  size=(vocab_size, 1)\n",
    "    gradOutsideVecs = np.outer(dscores, centerWordVec)  # J关于u的偏导数公式  size=(vocab_size, embedding_dim)\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dcefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "  \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    gradCenterVec =np.zeros(centerWordVec.shape)  # (embedding_size,1)\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)  # (vocab_size, embedding_size)\n",
    "    loss = 0.0\n",
    "\n",
    "    u_o = outsideVectors[outsideWordIdx]  # size=(embedding_size,1)\n",
    "    z = sigmoid(np.dot(u_o, centerWordVec))  # size=(1, )\n",
    "    loss -= np.log(z) # 损失函数的第一部分\n",
    "    gradCenterVec += u_o * (z - 1)   # J关于vc的偏导数的第一部分\n",
    "    gradOutsideVecs[outsideWordIdx] = centerWordVec * (z - 1)  # J关于u_o的偏导数计算\n",
    "\n",
    "    for i in range(K):\n",
    "        neg_id = indices[1 + i]\n",
    "        u_k = outsideVectors[neg_id]\n",
    "        z = sigmoid(-np.dot(u_k, centerWordVec))\n",
    "        loss -= np.log(z)\n",
    "        gradCenterVec += u_k * (1-z)\n",
    "        gradOutsideVecs[neg_id] += centerWordVec * (1 - z)\n",
    "\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cedd6d",
   "metadata": {},
   "source": [
    "### AirBnB Listing Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80d430",
   "metadata": {},
   "source": [
    "Real-time Personalization using Embeddings for Search Ranking at Airbnb\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/133566801\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/43295545"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
