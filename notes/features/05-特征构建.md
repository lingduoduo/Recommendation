05. 特征构建

机器学习模型一般只能处理向量化的数据，因此在建模过程中，需要将收集到的原始数据进行转化，构建出机器学习模型可以利用的数据形式（即向量化的数据），这个过程就是特征构建的过程。特征构建从收集到的机器学习模型的原始数据中提取出特征，将原始数据空间映射到新的特征空间，使得在新的特征空间中，模型能够更好地学习数据中的规律。 

很多经典的机器学习模型，比如logistic回归、线性模型、FM等需要进行精细的特征构建才能达到很好的效果。在如今的深度学习时代，由于数据量大，数据只需要进行简单的处理就可以灌入到深度学习模型中，最终可以获得比较好的效果。虽说如此，但是在很多时候特征构建是必须的，特征构建相当于通过人类的思考和理解，期望抓住问题的本质，辅助机器学习模型获得更好的效果。

不同类型的数据构建特征的方式是不一样的，上一章中我们提到了几类数据的预处理方式，下面我们分别对上一章中提到的几种数据类型，从离散特征、连续特征、时空特征、文本特征、富媒体特征等5类特征来介绍怎么进行特征构建。

随着Word2Vec及深度学习技术在推荐系统中的大规模应用，嵌入方法越来越受到欢迎，我们在本章的5.6节单独讲一下嵌入特征。文本、富媒体一般也可以直接用嵌入方法获得特征表示。

另外，我们在本章最后一节（即5.7节）讲解一下特征降维，这个是基于已有的特征构建维度更低的特征。更低维度的特征更利于模型处理，提升性能，同时降维的过程可能也是提升泛化能力的过程（降维可以理解为某种意义上的正则化）。

 5.1 离散特征构建

离散特征是非常常见的一类特征，推荐系统中的用户属性数据、物品属性数据中就包含大量的类别特征，如性别、学历、视频的类型、标签、导演、国别等等。对于离散特征，一般可以采用如下4种方式对特征进行编码（即特征构建）。

 **5.1.1 one-hot编码**

one-hot编码通常用于离散特征（也叫类别特征），如果某个类别特征有k类，我们将这k类固定一个序关系(随便什么序关系都无所谓，只是方便确认某个类在哪个位置)，我们可以将每个值映射为一个k维向量，其中这个值所在的分量为1，其他分量为0。比如性别进行编码的话，男可以编码为(1, 0)，女可以编码为(0, 1)。该方法当类别的数量很多时，特征空间会变得非常大。

当某个特征有多个类别时，这时one-hot编码可以拓展为n-hot编码。下面举个视频推荐系统中n-hot编码的例子。如果要将视频的标签进行n-hot编码，怎么做呢？我们知道每个视频可能有多个标签（比如恐怖、科幻等），编码的时候将该视频包含的所有标签对应的分量处设置为1，其他为0。这里的n是所有视频所有标签的总量，也即是全部可能的标签数量，一般是一个很大的数字（可能几万到几十万不等）。

 **5.1.2 散列****编码**

对于有些取值特别多的类别特征，使用one-hot编码得到的特征矩阵非常稀疏，如果再进行特征交叉（5.1.4会讲到），会使得特征维度爆炸式增长。特征散列的目标就是是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力，其优势在于实现简单，所需额外计算量小。

降低特征维度，也能加速算法训练与预测，降低内存消耗，但代价是通过哈希转换后学习到的模型变得很难检验(因为一般哈希函数是不可逆的)，我们很难对训练出的模型参数做出合理解释。特征散列的另一个问题是可能把多个原始特征哈希到相同的位置上，出现哈希冲突现象，但经验表明这种冲突对算法的精度影响很小，通过选择合适的hash函数也可以减少冲突概率。其实，每种程度的hash冲突也不一定是坏事，可能还可以提升模型的泛化能力。

 **5.1.3 计数****编码**

就是将所有样本中该类别出现的次数或者频次作为该特征的编码，这类方法对异常值比较敏感(拿电影的标签来说，很多电影包含“剧情”这个标签，计数编码会让剧情的编码值非常大)，也容易产生冲突(两个不同类别的编码一样，特别是对于出现很稀少的标签，编码值一样的概率非常大)。 

**5.1.4 离散特征之间交叉**

就是类别特征之间通过笛卡尔积(或者笛卡尔积的一个子集)生成新的特征，通过特征交叉有时可以捕捉细致的信息，对模型预测起到很重要的作用。这里举个例子，比如用用户地域与视频语言做交叉，大家肯定知道广东人一般更喜欢看粤语剧，那么这个交叉特征对预测粤语视频的点击是非常有帮助的。类别交叉一般需要对业务有较好的理解，需要足够多的领域知识，才可以构建好的交叉特征。

上面讲的是2个类别特征的交叉，当然还可以做3个、4个、甚至更多类别特征的交叉，两个类别交叉最多可以产生这两个类别基数的乘积这么多的新特征，所以交叉让模型的维数爆炸性增长，增加了模型训练的难度。同时，更多的特征需要更多的样本来支撑，否则极容易过拟合。对于样本量不够多的场景，不建议采用超出2个类别的交叉，也不建议用2个基数特别大的类别进行特征交叉。 

另外，对于有序离散特征，我们可以用0、1、2、... 等自然数来为他们编码，自然数的大小关系保证了它们之间的序关系。

**5.2 连续特征构建**

连续型数据是机器学习算法直接可以使用的数据，对于连续型数据，我们一般可以通过如下几种方式来构建特征。

**5.2.1 直接使用**

机器学习算法是可以直接处理数值特征的，数值特征可能需要利用上一章讲到的数据预处理中的部分方法进行处理后再灌给模型使用。 

**5.2.2 离散化**

有时连续特征需要进行离散化处理，比如视频在一段时间内的播放量对于视频点击CTR预估可能是一个重要的特征，因为播放次数跟视频的热度有很强的相关性，但是如果不同视频的播放次数的数量级相差巨大(实际情况确实是这样，热门视频比冷门视频播放量大若干个数量级)，该特征就很难起作用(比如 LR 模型，模型往往只对比较大的特征值敏感)。对于这种情况，通常的解决方法是进行分桶。分桶操作可以看作是对数值变量的离散化，之后再进行 one-hot 编码。 

分桶的数量和宽度可以根据业务知识和经验来确定，一般有三种分桶方式：(1) 等距分桶，每个桶的长度是固定的，这种方式适用于样本分布比较均匀的情况；(2) 等频分桶，即每个桶里样本量一样多，但也会出现特征值差异非常大的样本被放在一个桶中的情况；(3) 模型分桶，使用模型找到最佳分桶，例如利用聚类的方式将特征分成多个类别，或者利用树模型，这种非线性模型天生具有对连续型特征切分的能力，利用特征分割点进行离散化。

分桶是离散化的常用方法，连续特征离散化是有一定价值的：离散化之后得到的稀疏向量，运算速度更快，计算结果易于存储。离散化之后的特征对于异常值也具有更强的鲁棒性。需要注意的是：(1)  每个桶内都有足够多的样本，否则不具有统计意义；(2)  每个桶内的样本尽量分布均匀。

 **5.2.3 特征交叉**

对于连续特征x、y，通过非线性函数 f 的作用，我们将 z=f(x,y) 作为交叉特征，一般 f 可以是多项式函数，最常用的交叉函数是 f=xy ，即两个特征对应的值直接相乘。通过特征交叉可以为模块提供更多的非线性，可以更细致地拟合输入输出之间的复杂关系，但非线性交叉让模型计算处理变得更加困难。

我们也可以进行类别特征与数值特征之间的交叉，只不过这种交叉一般是统计某个类别具体值对应的数值特征的统计量(次数、和、均值、最值、方差等等)。拿电影的语言和用户的年龄两个特征交叉来说，我们可以分别统计看过语言是中文、英语等的电影中用户的平均年龄。根据大家的经验，我们知道年轻人受教育程度高，英语会更好，所以看过英语电影的人的平均年龄比看中文的平均年龄低。这类特征的交叉也需要基于具体业务场景及领域知识来做，否则获得的交叉特征可能无效，甚至给模型引入噪音。

 **5.3 时空特征构建**

时间和地理位置也是两类非常重要的特征，下面分别来说明怎么将它们转化为模型特征。 

对于时间来说，一般有如下几种转换为特征的方式： 

**5.3.1 转化为数值**

比如将时间转化为从某个基准时间开始到该时间经历的秒数、天数、月数、年数等。用更大的单位相当于对小单位四舍五入(比如用到当前时间经历的年数作为特征，那么不足一年的时间都忽略了)，当然也可以不用四舍五入，这时用小数就可以，比如到现在经历了4.5年。

**5.3.2 将时间离散化**

比如我们可以根据当前时间是不是节假日，将时间离散化为0-1二值(1是假日，0是工作日)。再比如如果我们构建的模型是与周期性相关的，我们可能只需要取时间中的周几这个量，那么时间就可以离散化为0-6七个数字(0代表星期天，1代表星期一，如此类推)。 

对于地理位置来说，我们有行政区划表示，还有经纬度表示，以及到某个固定点的距离等表示方式，下面分别说明。

**1) 行政区划表示**

典型的是用户所在地区，因为地区是固定的，数量也是有限的，这时地理位置就转化为离散特征了。

**2) 经纬度表示**

地理位置也可以用经纬度表示，这时每个位置就转化为一个2维向量了(一个分量是经度，另一个分量是纬度)。

**3) 距离表示**

对于像美团、滴滴这类基于LBS服务的产品，一般用商家或者司机到用户的距离来表示位置，这时地理位置就转化为一个一维的数值了。

**5.4 文本特征构建**

对于文本一般可以用NLP等相关技术进行处理转化为数值特征。对于新闻资讯等文档，可以采用TF-IDF、LDA等将每篇文档转化为一个高维的向量表示。或者基于Word2Vec等相关技术将整篇文档嵌入(doc2vec)到一个低维的稠密向量空间。

**5.5 富媒体特征构建**

对于图片、音频、视频等富媒体，一般也可以基于相关领域的技术获得对应的向量表示，这种向量表示就可以作为富媒体的特征了。这里不细介绍，感兴趣的读者可以自行搜索学习。 

**5.6 嵌入特征构建**

上面文本、富媒体中提到的嵌入技术是非常重要的一类提取特征的技术。所谓嵌入，就是将高维空间的向量投影到低维空间，降低数据的稀疏性，减少维数灾难，同时提升数据表达的鲁棒性。随着Word2Vec及深度学习技术的流行，嵌入特征越来越重要。下面我们来举个视频推荐系统中视频嵌入的例子。视频的嵌入分为基于内容的嵌入和基于行为的嵌入。

 基于内容的嵌入使用标的物属性信息(如视频的标题、标签、演职员、海报图，视频、音频等信息)，通过 NLP、CV、深度学习等技术生成嵌入向量。

基于行为的嵌入是基于用户与标的物的交互行为数据生成嵌入。用户在一段时间中前后点击的视频存在一定的相似性，通常会表现出对某类型视频的兴趣偏好，可能是同一个风格类别，或者是相似的话题等，因此我们将一段时间内用户点击的视频 id 序列作为训练数据(id可以类比word，这个序列类比为一篇文档)，使用 skip-gram 模型学习视频的嵌入特征。由于用户点击行为具有相关关系，因此得到的嵌入特征有很好的聚类效果，使得在特征空间中，同类目的视频聚集在一起，相似类目的视频在空间中距离相近。作者在之前的视频推荐业务中，就是采用这种嵌入，通过嵌入向量计算视频之间的相似性，然后进行相似推荐，有不错的效果。

对于用户嵌入，我们也可以有很多方法，下面是两种比较基础的方法。

可以将一段时间内用户点击过的视频的平均嵌入特征向量作为该用户的嵌入特征，这里的“平均”可以是简单的算术平均，可以是element-wise max，也可以是根据视频的热度和时间属性等进行加权平均或者尝试用 RNN 替换掉平均操作。我们可以通过选择时间周期的长短来刻画用户的长期兴趣嵌入和短期兴趣嵌入。

另外参考YouTube推荐系统(Deep Neural Networks for YouTube Recommendations)的思路，我们可以把推荐问题等价为一个多类别分类问题，使用 softmax 损失函数学习一个 DNN 模型，最终预测在某一时刻某一上下文信息下用户观看的下一个视频的类别，最后把训练好的 DNN 模型最后一层隐含层输出作为用户的嵌入向量，有兴趣的读者可以参考我的另一本书《构建企业级推荐系统：算法、工程实现与案例分析》中10.3.1节的介绍。 

**5.7 特征降维**

对于维度比较高的特征，我们可以通过降维进行处理，构建更低维度的特征。我们可以基于全部模型特征进行降维，也可以基于部分特征，甚至是单个特征进行降维。其实上面提到的嵌入方法可以看成是一种降维的形式。

在矩阵分解推荐算法中，当我们选定了隐因子k后，就可以将用户行为矩阵分解为用户矩阵和物品矩阵，这时就可以获得用户特征向量和物品特征向量，他们都是k维的，在没有降维之前，行为矩阵的行和列是可以分别用作用户向量和物品向量的，只不过没有进行矩阵分解时，用户向量（即用户行为矩阵的行）和物品向量（即用户行为矩阵的列）都是维数很高的，并且他们维数还不一样（用户向量的维数是物品的个数，物品向量的维数是用户个数）。关于矩阵分解，读者可以参考我的另一本书《构建企业级推荐系统：算法、工程实现与案例分析》第6章的介绍，这里不再赘述。

降维的方法可以用PCA或者KMeans聚类。这里简单说一下KMeans聚类降维。如果某个特征有100维，我们用KMeans聚类为10类，那么每个样本都可以归为1-10中的某一类，这就是一个离散特征了，这时就可以采用one-hot编码进行降维，降维后特征就是10维了。 

**总结**

本章讲解了推荐系统中进行特征构建的一般思路和方法，这些方法基本覆盖了推荐系统中可能遇到的各种数据形式，掌握这些特征构建的方法有利于读者更好地构建推荐模型。当读者用机器学习模型去构建推荐系统时（比如利用logistic回归去做排序），掌握这些特征构建方法，可以帮助读者更快地构建一个可用的推荐模型。

特征构建不是一个纯技术的活，如果读者对需要使用机器学习模型的业务场景不了解，是很难构建出很好的特征的，可能会走很多弯路。所以这里提醒一下读者，在做某个机器学习项目时，需要深刻理解问题背后的业务背景，这将有助于你更好地选择合适的特征。